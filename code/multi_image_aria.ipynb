{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91773b2a-297c-4b3d-92cc-9c232c0ef9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def open_images_from_directory(directory):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    img = []\n",
    "    for file in (files):\n",
    "        # Check if the file is an image\n",
    "        if file.endswith('.jpg'):\n",
    "            try:\n",
    "                image_path = os.path.join(directory, file)\n",
    "                img.append(Image.open(image_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Error opening image {file}: {e}\")\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f8f1a8-c533-4438-a5af-34f4648a96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, AutoModelForCausalLM\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"CPU\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ffc8c5e-8581-4111-b8ac-a2936e2880e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_messages(image_array):\n",
    "    # Generate image content based on the length of image_array\n",
    "    image_contents = [{\"type\": \"image\"} for _ in image_array]\n",
    "    \n",
    "    # Add the text message to describe the images\n",
    "    text_content = {\"type\": \"text\", \"text\": \"Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\"}\n",
    "    \n",
    "    # Combine image contents and text content\n",
    "    messages = [{\"role\": \"user\", \"content\": image_contents + [text_content]}]\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "345d8eaf-5b1f-4a6a-90ed-9856224630df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x71BE6C6A2A70>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x71BD99B63E80>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x71BD99B63EB0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x71BD99B63FA0>]\n",
      "[{'role': 'user', 'content': [{'type': 'image'}, {'type': 'image'}, {'type': 'image'}, {'type': 'image'}, {'type': 'text', 'text': \"Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\"}]}]\n"
     ]
    }
   ],
   "source": [
    "directory = '/home/ttyh/hot3d/hot3d/dataset/mcq/all_frames/Pick up_P0001_a68492d5_new_8'\n",
    "\n",
    "img = open_images_from_directory(directory)\n",
    "print(img)\n",
    "\n",
    "messages = create_input_messages(img)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab58c946-f4a4-4e3c-90a9-a74aebd49cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x71BD99B63FD0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x71BD99B0ABC0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x71BD99B0AC50>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x71BD99B0ADD0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x71BD99B0A950>]\n",
      "[{'role': 'user', 'content': [{'type': 'image'}, {'type': 'image'}, {'type': 'image'}, {'type': 'image'}, {'type': 'image'}, {'type': 'text', 'text': \"Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\"}]}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set your directory path here\n",
    "directory = '/home/ttyh/hot3d/hot3d/dataset/Labelled/Videos/new_frames/Pick up_P0001_a68492d5_new_1'\n",
    "\n",
    "img = open_images_from_directory(directory)\n",
    "print(img)\n",
    "\n",
    "messages = create_input_messages(img)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837dfc3d-a8c7-4069-9ca2-c27c3bf680c8",
   "metadata": {},
   "source": [
    "**SmolVLM-Instruct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc56d045-eb4d-4988-b3a2-e1e580af69fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d028105292462caef1c7ae362099eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33631caacb5e431da3f00e5839584bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/429 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc22fc1ca3d4e5cb2e0f3f14d6128c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/486 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf7d209326f4830bd9df44494912ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02237dbce127474a9b02564dfb530612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a61929588a541b78d2077dab020a4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e171d48dca554ec7947d9aabb180f7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5613f2fde2a74a219076d2a2f2904b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/92.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f440e67631024809b0a89a37453f26b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: image_seq_len. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fe25045a1646ebaf64d195e18bd060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/7.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0a133290764135a98eeb883bf89032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x71be6e46b4c0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ttyh/hot3d/hot3d/.pixi/envs/default/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize processor, model and load PEFT adapter\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolVLM-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n",
    ").to(DEVICE)\n",
    "model.load_adapter(\"HuggingFaceTB/SmolVLM-Instruct-DPO\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "219e75ea-42ee-48c9-b416-2b79af977d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:<image>Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\n",
      "Assistant: The user is performing a pick up task. The item being picked up is the milk carton.\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=img, return_tensors=\"pt\")\n",
    "inputs = inputs.to(DEVICE)\n",
    "\n",
    "# Generate outputs\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "generated_texts = processor.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "print(generated_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16445c8-ccb1-427d-8bf3-f1c7a37bf287",
   "metadata": {},
   "source": [
    "**Idefics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3fbdc5c-e20b-4643-ae8f-585f0c3014a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only 2 images\n",
    "directory = \"/home/ttyh/hot3d/hot3d/dataset/Labelled/Videos/new_frames/Pick up_P0001_a68492d5_new_8\"\n",
    "\n",
    "img = open_images_from_directory(directory)\n",
    "img = img[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf4a77f-b0ae-40c4-bf02-f59b422c6adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': [{'type': 'image'}, {'type': 'image'}, {'type': 'text', 'text': \"Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\"}]}]\n"
     ]
    }
   ],
   "source": [
    "messages = create_input_messages(img)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b0dcf31-035d-4041-9500-9d65b5c7da62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb09dbf44e044f2fa1789c08040376b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/Idefics3-8B-Llama3\")\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceM4/Idefics3-8B-Llama3\", torch_dtype=torch.bfloat16\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfea48b2-73a4-4e16-ae88-fb22010e3823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"User:<image>Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\\nAssistant: The task is a place task. The user is pointing at a red, white, and blue carton that is on a brown cart. The carton is being placed on the cart.\"]\n"
     ]
    }
   ],
   "source": [
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=img, return_tensors=\"pt\")\n",
    "#inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "inputs = inputs.to(DEVICE)\n",
    "\n",
    "# Generate\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc8c6915-c71e-4a33-b222-0bc579d478bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"User:<image>Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\\nAssistant: The task is a place task. The rubik's cube is being placed on the table.\"]\n"
     ]
    }
   ],
   "source": [
    "#CUDA out of memory for 3 even if using float16\n",
    "directory = '/home/ttyh/hot3d/hot3d/dataset/mcq/all_frames/Pick up cup from table_56743856578795.00'\n",
    "img = open_images_from_directory(directory)\n",
    "img = img[:3]\n",
    "messages = create_input_messages(img)\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(model.device, torch.float16)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c6a6507-6a0a-4542-9d3f-eda3db983a6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': [{'type': 'image'}, {'type': 'image'}, {'type': 'image'}, {'type': 'text', 'text': \"Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\"}]}]\n",
      "[\"User:<image>Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\\nAssistant: The task is placing. The user is putting a milk carton on the shelf.\"]\n"
     ]
    }
   ],
   "source": [
    "#CUDA out of memory with 3 images\n",
    "directory = \"/home/ttyh/hot3d/hot3d/dataset/Labelled/Videos/new_frames/Pick up_P0001_a68492d5_new_8\"\n",
    "\n",
    "img = open_images_from_directory(directory)\n",
    "img = img[1:]\n",
    "\n",
    "messages = create_input_messages(img)\n",
    "print(messages)\n",
    "\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=img, return_tensors=\"pt\")\n",
    "#inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "inputs = inputs.to(DEVICE)\n",
    "\n",
    "# Generate\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecef077-a56a-43c8-85e8-571d3b09c7fd",
   "metadata": {},
   "source": [
    "**llava-hf/llava-onevision-qwen2-7b-ov-hf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a756d380-235d-4070-ae99-9cc587228bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': [{'type': 'image'}, {'type': 'image'}, {'type': 'image'}, {'type': 'image'}, {'type': 'text', 'text': \"Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\"}]}]\n"
     ]
    }
   ],
   "source": [
    "directory = \"/home/ttyh/hot3d/hot3d/dataset/Labelled/Videos/new_frames/Pick up_P0001_a68492d5_new_8\"\n",
    "\n",
    "img = open_images_from_directory(directory)\n",
    "\n",
    "messages = create_input_messages(img)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d050bde-1a3c-4b39-87a9-a943892898e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d932bf9375d54d989a581d0a39df2cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n",
    "\n",
    "# Load the model in half-precision\n",
    "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa141cca-c879-47fa-b081-cbb72e5a1826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(images=img, text=prompt, padding=True, return_tensors=\"pt\").to(model.device, torch.float16)\n",
    "inputs = inputs.to(DEVICE)\n",
    "\n",
    "# Generate outputs\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "generated_texts = processor.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "print(generated_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d8569-2e1d-4db9-b523-5808d4c1cd59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CUDA out of memory for 5 images\n",
    "directory = '/home/ttyh/hot3d/hot3d/dataset/Labelled/Videos/new_frames/Pick up_P0001_a68492d5_new_1'\n",
    "\n",
    "img = open_images_from_directory(directory)\n",
    "print(img)\n",
    "\n",
    "messages = create_input_messages(img)\n",
    "print(messages)\n",
    "\n",
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(images=img, text=prompt, padding=True, return_tensors=\"pt\").to(model.device, torch.float16)\n",
    "inputs = inputs.to(DEVICE)\n",
    "\n",
    "# Generate outputs\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "generated_texts = processor.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "print(generated_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e858cff-bd87-42e8-b482-405fd26071ae",
   "metadata": {},
   "source": [
    "**\"microsoft/Phi-3.5-vision-instruct\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5001df64-c3cd-4703-bf08-8e9ebd5ffb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f86ce526d5747e59c09d00020d49b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uril/hot3d/hot3d/.pixi/envs/default/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:520: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM \n",
    "model_id = \"microsoft/Phi-3.5-vision-instruct\" \n",
    "\n",
    "# Note: set _attn_implementation='eager' if you don't have flash_attn installed\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id, \n",
    "  device_map=\"cuda\", \n",
    "  trust_remote_code=True, \n",
    "  torch_dtype=\"auto\", \n",
    "  _attn_implementation='flash_attention_2'    \n",
    ")\n",
    "\n",
    "# for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\n",
    "processor = AutoProcessor.from_pretrained(model_id, \n",
    "  trust_remote_code=True, \n",
    "  num_crops=4\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd7dc30-800e-4311-8934-d9c7286125ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uril/hot3d/hot3d/.pixi/envs/default/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user is performing a pick task. The item being picked up is the red object, which appears to be a remote control.\n"
     ]
    }
   ],
   "source": [
    "placeholder = ''\n",
    "for i in range(len(img)):\n",
    "    placeholder += f\"<|image_{i+1}|>\\n\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": placeholder+\"Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\"},\n",
    "]\n",
    "\n",
    "prompt = processor.tokenizer.apply_chat_template(\n",
    "  messages, \n",
    "  tokenize=False, \n",
    "  add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = processor(prompt, img, return_tensors=\"pt\").to(\"cuda:0\") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 1000, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "generate_ids = model.generate(**inputs, \n",
    "  eos_token_id=processor.tokenizer.eos_token_id, \n",
    "  **generation_args\n",
    ")\n",
    "\n",
    "# remove input tokens \n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "response = processor.batch_decode(generate_ids, \n",
    "  skip_special_tokens=True, \n",
    "  clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3841c9-1c94-44fc-91de-feef1b67bf56",
   "metadata": {},
   "source": [
    "**Playing with memory_profiler to compare the memory usage and time taken between models**\n",
    "\n",
    "**since SmolVLM and Phi3.5 is able to deal with the same 5 images data, i will use those first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c93557b4-0e97-421a-ad87-bc6af7df0597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: image_seq_len. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x73C525729DB0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x73C525729B10>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x73C5257297B0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x73C525729A80>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x73C525728C40>]\n",
      "[{'role': 'user', 'content': [{'type': 'image'}, {'type': 'image'}, {'type': 'image'}, {'type': 'image'}, {'type': 'image'}, {'type': 'text', 'text': \"Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\"}]}]\n"
     ]
    }
   ],
   "source": [
    "# Initialize processor, model and load PEFT adapter\n",
    "processor_a = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "model_a = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolVLM-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n",
    ").to(DEVICE)\n",
    "model_a.load_adapter(\"HuggingFaceTB/SmolVLM-Instruct-DPO\")\n",
    "\n",
    "# Set your directory path here\n",
    "directory = '/home/ttyh/hot3d/hot3d/dataset/Labelled/Videos/new_frames/Pick up_P0001_a68492d5_new_1'\n",
    "\n",
    "img = open_images_from_directory(directory)\n",
    "print(img)\n",
    "\n",
    "messages = create_input_messages(img)\n",
    "print(messages)\n",
    "\n",
    "# Prepare inputs\n",
    "prompt_a = processor_a.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs_a = processor_a(text=prompt_a, images=img, return_tensors=\"pt\")\n",
    "inputs_a = inputs_a.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "641b5b74-283a-4f0c-b80b-4d6407594b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce1e1444aeb4ee4b51a6ebd3ec6b3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uril/hot3d/hot3d/.pixi/envs/default/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:520: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "/home/uril/hot3d/hot3d/.pixi/envs/default/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user is performing a pick task. The item being picked up is the red object, which appears to be a remote control.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM \n",
    "model_id = \"microsoft/Phi-3.5-vision-instruct\" \n",
    "\n",
    "# Note: set _attn_implementation='eager' if you don't have flash_attn installed\n",
    "model_b = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id, \n",
    "  device_map=\"cuda\", \n",
    "  trust_remote_code=True, \n",
    "  torch_dtype=\"auto\", \n",
    "  _attn_implementation='flash_attention_2'    \n",
    ")\n",
    "\n",
    "# for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\n",
    "processor_b = AutoProcessor.from_pretrained(model_id, \n",
    "  trust_remote_code=True, \n",
    "  num_crops=4\n",
    ") \n",
    "\n",
    "placeholder = ''\n",
    "for i in range(len(img)):\n",
    "    placeholder += f\"<|image_{i+1}|>\\n\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": placeholder+\"Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\"},\n",
    "]\n",
    "\n",
    "prompt_b = processor_b.tokenizer.apply_chat_template(\n",
    "  messages, \n",
    "  tokenize=False, \n",
    "  add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs_b = processor_b(prompt_b, img, return_tensors=\"pt\").to(\"cuda:0\") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bc80aa6-6086-4711-a0f7-f334d4d04c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x79F685C91150>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x79F69647B640>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x79F696478070>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x79F6964788B0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1408x1408 at 0x79F69647B6D0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some kwargs in processor config are unused and will not have any effect: image_seq_len. \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c8eb13682b437f93d108c781effaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A Setup Time: 2.7268 seconds, Memory Usage: 4890.77 MiB\n",
      "Model A Input Preparation Time: 0.6574 seconds, Memory Usage: 1391.22 MiB\n",
      "Model B Setup Time: 2.9334 seconds, Memory Usage: 5083.75 MiB\n",
      "Model B Input Preparation Time: 0.2245 seconds, Memory Usage: 1087.25 MiB\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import gc\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "directory = '/home/ttyh/hot3d/hot3d/dataset/Labelled/Videos/new_frames/Pick up_P0001_a68492d5_new_1'\n",
    "\n",
    "img = open_images_from_directory(directory)\n",
    "print(img)\n",
    "\n",
    "\n",
    "# Function to measure memory for model loading\n",
    "def load_a(model_cls, processor_cls, model_id):\n",
    "    model = model_cls.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",).to(DEVICE)\n",
    "    model.load_adapter(\"HuggingFaceTB/SmolVLM-Instruct-DPO\")\n",
    "    processor = processor_cls.from_pretrained(model_id, trust_remote_code=True)\n",
    "    return model, processor\n",
    "\n",
    "# Function to prepare inputs\n",
    "def prepare_inputs_a(img, processor_a):\n",
    "    messages = create_input_messages(img)\n",
    "    prompt_a = processor_a.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    return processor_a(text=prompt_a, images=img, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "def load_b(model_cls, processor_cls, model_id):\n",
    "    model_b = model_cls.from_pretrained(\n",
    "      model_id, \n",
    "      device_map=\"cuda\", \n",
    "      trust_remote_code=True, \n",
    "      torch_dtype=\"auto\", \n",
    "      _attn_implementation='flash_attention_2'    \n",
    "    )\n",
    "    processor_b = processor_cls.from_pretrained(model_id, \n",
    "      trust_remote_code=True, \n",
    "      num_crops=4\n",
    "    ) \n",
    "    return model_b, processor_b\n",
    "\n",
    "def prepare_inputs_b(img, processor_b):\n",
    "    placeholder = ''\n",
    "    for i in range(len(img)):\n",
    "        placeholder += f\"<|image_{i+1}|>\\n\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": placeholder+\"Analyze this sequence of frames where the red spot shows the user's eye gaze. Identify whether the user is performing a pick or place task. If the task is picking, specify the item being picked up. If the task is placing, describe what is being placed and its destination.\"},\n",
    "    ]\n",
    "    prompt_b = processor_b.tokenizer.apply_chat_template(\n",
    "      messages, \n",
    "      tokenize=False, \n",
    "      add_generation_prompt=True\n",
    "    )\n",
    "    return processor_b(prompt_b, img, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "# Measure memory for Model A setup\n",
    "start_time = time.time()\n",
    "mem_usage_a, (model_a, processor_a) = memory_usage(\n",
    "    (lambda: load_a(AutoModelForVision2Seq, AutoProcessor, \"HuggingFaceTB/SmolVLM-Instruct\")),\n",
    "    retval=True,\n",
    "    max_usage=True,\n",
    ")\n",
    "end_time = time.time()\n",
    "setup_time_a = end_time - start_time\n",
    "\n",
    "# Prepare inputs for Model A\n",
    "start_time = time.time()\n",
    "mem_usage_inputs_a, inputs_a = memory_usage(\n",
    "    (lambda: prepare_inputs_a(img, processor_a)),\n",
    "    retval=True,\n",
    "    max_usage=True,\n",
    ")\n",
    "end_time = time.time()\n",
    "inputs_time_a = end_time - start_time\n",
    "\n",
    "# Clear memory before Model B setup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Measure memory for Model B setup\n",
    "start_time = time.time()\n",
    "mem_usage_b, (model_b, processor_b) = memory_usage(\n",
    "    (lambda: load_b(AutoModelForCausalLM, AutoProcessor, \"microsoft/Phi-3.5-vision-instruct\")),\n",
    "    retval=True,\n",
    "    max_usage=True,\n",
    ")\n",
    "end_time = time.time()\n",
    "setup_time_b = end_time - start_time\n",
    "\n",
    "# Prepare inputs for Model B\n",
    "start_time = time.time()\n",
    "mem_usage_inputs_b, inputs_b = memory_usage(\n",
    "    (lambda: prepare_inputs_b(img, processor_b)),\n",
    "    retval=True,\n",
    "    max_usage=True,\n",
    ")\n",
    "end_time = time.time()\n",
    "inputs_time_b = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"Model A Setup Time: {setup_time_a:.4f} seconds, Memory Usage: {mem_usage_a:.2f} MiB\")\n",
    "print(f\"Model A Input Preparation Time: {inputs_time_a:.4f} seconds, Memory Usage: {mem_usage_inputs_a:.2f} MiB\")\n",
    "print(f\"Model B Setup Time: {setup_time_b:.4f} seconds, Memory Usage: {mem_usage_b:.2f} MiB\")\n",
    "print(f\"Model B Input Preparation Time: {inputs_time_b:.4f} seconds, Memory Usage: {mem_usage_inputs_b:.2f} MiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91b9703c-f56e-4abe-8d3f-d0438e452640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A Inference Time: 1.6406 seconds, Memory Usage: 1860.91 MiB\n",
      "Model B Inference Time: 1.0146 seconds, Memory Usage: 1860.91 MiB\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import gc\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "def generate_a(processor_a, model_a, inputs_a):\n",
    "    id_a = model_a.generate(**inputs_a, max_new_tokens=500)[:, inputs_a[\"input_ids\"].shape[1]:]\n",
    "    return processor_a.batch_decode(id_a, skip_special_tokens=True)\n",
    "\n",
    "def generate_b(generation_args, processor_b, model_b, inputs_b):\n",
    "    id_b = model_b.generate(\n",
    "        **inputs_b,\n",
    "        eos_token_id=processor_b.tokenizer.eos_token_id,\n",
    "        **generation_args\n",
    "    )[:, inputs_b[\"input_ids\"].shape[1]:]\n",
    "    return processor_b.batch_decode(id_b, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "# Measure inference time and memory usage for Model A\n",
    "start_time = time.time()\n",
    "mem_usage_a, output_a = memory_usage(\n",
    "    (lambda: generate_a(processor_a, model_a, inputs_a)),\n",
    "    retval=True,\n",
    "    max_usage=True,\n",
    ")\n",
    "end_time = time.time()\n",
    "inference_time_a = end_time - start_time\n",
    "\n",
    "# Clear memory before measuring Model B\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Measure inference time and memory usage for Model B\n",
    "start_time = time.time()\n",
    "mem_usage_b, output_b = memory_usage(\n",
    "    (lambda: generate_b(generation_args, processor_b, model_b, inputs_b)),\n",
    "    retval=True,\n",
    "    max_usage=True,\n",
    ")\n",
    "end_time = time.time()\n",
    "inference_time_b = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"Model A Inference Time: {inference_time_a:.4f} seconds, Memory Usage: {mem_usage_a:.2f} MiB\")\n",
    "print(f\"Model B Inference Time: {inference_time_b:.4f} seconds, Memory Usage: {mem_usage_b:.2f} MiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a019abed-3f9f-4913-8ff8-14bbc514bff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' The user is performing a pick up task. The item being picked up is the milk carton.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ccb12867-775d-4e7b-b056-df68e67ebe6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The user is performing a pick task. The item being picked up is the red object, which appears to be a remote control.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
