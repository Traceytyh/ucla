{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "YOLOv8s-seg summary: 261 layers, 11,790,483 parameters, 0 gradients, 42.7 GFLOPs\n",
      "\n",
      "image 1/1 /home/ttyh/hot3d/hot3d/dataset/images/bowl.jpg: 480x640 38 objects, 44.1ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from ultralytics import SAM, FastSAM\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the model on GPU if available\n",
    "model = FastSAM(\"FastSAM-s.pt\").to(device)\n",
    "\n",
    "# Perform inference with the model running on GPU\n",
    "#results = model(source=image_path, device=device, stream=True)\n",
    "\n",
    "# Display model information (optional)\n",
    "model.info()\n",
    "\n",
    "# Define thresholds for filtering bounding boxes\n",
    "MAX_AREA_THRESHOLD = 8000  # Maximum area of a bounding box to consider\n",
    "MIN_AREA_THRESHOLD = 300   # Minimum area of a bounding box to consider\n",
    "MAX_ASPECT_RATIO = 2.0     # Maximum aspect ratio to consider a segment elongated\n",
    "MIN_WHITE_INTENSITY = 125  # Minimum mean intensity to consider a blob \"white\"\n",
    "\n",
    "# Path to the images directory\n",
    "image_folder = '/home/ttyh/hot3d/hot3d/dataset/images'\n",
    "output_folder = '/home/ttyh/hot3d/hot3d/dataset/cropped_objects'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process each image in the folder\n",
    "for image_name in os.listdir(image_folder):\n",
    "    image_path = os.path.join(image_folder, image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Run inference on the current frame to get bounding boxes\n",
    "    results = model(source=image_path, stream=True)\n",
    "\n",
    "    for r in results:\n",
    "        masks = r.masks  # Get segmentation masks\n",
    "\n",
    "        # Extract bounding boxes from masks\n",
    "        for mask in masks.xy:\n",
    "            # Convert the list of coordinates to a NumPy array (polygon format)\n",
    "            mask_polygon = np.array(mask, dtype=np.int32).reshape((-1, 1, 2))\n",
    "\n",
    "            # Fit a bounding rectangle to the mask and calculate aspect ratio and area\n",
    "            x, y, w, h = cv2.boundingRect(mask_polygon)\n",
    "            area = w * h  # Calculate bounding box area\n",
    "\n",
    "            # Filter out unwanted bounding boxes based on area and aspect ratio\n",
    "            if y < 75 or h == 0 or w == 0:\n",
    "                continue\n",
    "            aspect_ratio = max(w / h, h / w)\n",
    "\n",
    "            if area > MAX_AREA_THRESHOLD or area < MIN_AREA_THRESHOLD:\n",
    "                continue  # Skip this bounding box if it's too large or too small\n",
    "\n",
    "            # Crop the detected object from the original image\n",
    "            cropped_object = image[y:y+h, x:x+w]\n",
    "            \n",
    "            # Save the cropped object\n",
    "            #cropped_filename = os.path.join(output_folder, f\"cropped_{image_name}_{x}_{y}.jpg\")\n",
    "            #cv2.imwrite(cropped_filename, cropped_object)\n",
    "            #print(f\"Saved cropped object: {cropped_filename}\")\n",
    "\n",
    "            # Draw the bounding box on the original image (green color)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Display the image with bounding boxes\n",
    "        cv2.imshow(\"Detected Objects\", image)\n",
    "\n",
    "        # Break if 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Now, you can process the cropped images with YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 608x640 1 cake, 246.4ms\n",
      "Speed: 2.4ms preprocess, 246.4ms inference, 0.9ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Detected in cropped_bowl.jpg_282_198.jpg: cake, Confidence: 0.78, BBox: [[0.17151831090450287, 2.0732829570770264, 74.72217559814453, 65.25249481201172]]\n",
      "\n",
      "0: 608x640 1 cake, 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Detected in cropped_bowl.jpg_478_182.jpg: cake, Confidence: 0.62, BBox: [[0.13836364448070526, 1.267905354499817, 66.3194808959961, 61.257545471191406]]\n",
      "\n",
      "0: 352x640 1 apple, 7.3ms\n",
      "Speed: 1.4ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Detected in cropped_bowl.jpg_374_170.jpg: apple, Confidence: 0.50, BBox: [[0.8169276118278503, 0.5461934208869934, 82.26480102539062, 41.80857849121094]]\n",
      "\n",
      "0: 640x640 1 banana, 6.6ms\n",
      "Speed: 2.0ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected in cropped_bowl.jpg_314_234.jpg: banana, Confidence: 0.76, BBox: [[3.20306396484375, 4.376438140869141, 173.30397033691406, 176.03306579589844]]\n",
      "\n",
      "0: 640x640 1 orange, 1 cake, 6.7ms\n",
      "Speed: 1.9ms preprocess, 6.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected in cropped_bowl.jpg_218_178.jpg: orange, Confidence: 0.27, BBox: [[1.635424017906189, 1.459896445274353, 49.61262893676758, 49.053367614746094]]\n",
      "Detected in cropped_bowl.jpg_218_178.jpg: cake, Confidence: 0.26, BBox: [[0.986311674118042, 1.605094075202942, 49.55539321899414, 48.601016998291016]]\n",
      "\n",
      "0: 448x640 (no detections), 239.6ms\n",
      "Speed: 1.5ms preprocess, 239.6ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 6.9ms\n",
      "Speed: 1.2ms preprocess, 6.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 (no detections), 242.5ms\n",
      "Speed: 2.0ms preprocess, 242.5ms inference, 0.3ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x480 (no detections), 6.2ms\n",
      "Speed: 1.4ms preprocess, 6.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x416 (no detections), 243.8ms\n",
      "Speed: 1.6ms preprocess, 243.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 544x640 1 donut, 1 cake, 243.6ms\n",
      "Speed: 2.3ms preprocess, 243.6ms inference, 0.8ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Detected in cropped_bread.jpg_274_206.jpg: donut, Confidence: 0.51, BBox: [[0.4292449951171875, 0.46944427490234375, 77.0833740234375, 67.33672332763672]]\n",
      "Detected in cropped_bread.jpg_274_206.jpg: cake, Confidence: 0.34, BBox: [[0.6762771606445312, 0.8135223388671875, 76.96336364746094, 66.10612487792969]]\n",
      "\n",
      "0: 352x640 1 banana, 6.3ms\n",
      "Speed: 2.0ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Detected in cropped_bread.jpg_326_242.jpg: banana, Confidence: 0.71, BBox: [[0.6498363614082336, 0.9268264770507812, 232.52044677734375, 120.10397338867188]]\n",
      "\n",
      "0: 640x608 1 orange, 242.5ms\n",
      "Speed: 2.7ms preprocess, 242.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Detected in cropped_bread.jpg_406_194.jpg: orange, Confidence: 0.51, BBox: [[1.9469351768493652, 0.7834348082542419, 61.25819396972656, 47.31901168823242]]\n",
      "\n",
      "0: 416x640 1 orange, 231.7ms\n",
      "Speed: 1.6ms preprocess, 231.7ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Detected in cropped_bread.jpg_302_134.jpg: orange, Confidence: 0.29, BBox: [[1.641082763671875, 0.3795398473739624, 69.70758819580078, 44.5496940612793]]\n",
      "\n",
      "0: 608x640 (no detections), 8.1ms\n",
      "Speed: 3.0ms preprocess, 8.1ms inference, 0.3ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x320 1 cup, 1 toilet, 247.6ms\n",
      "Speed: 1.4ms preprocess, 247.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Detected in cropped_bread.jpg_566_318.jpg: toilet, Confidence: 0.30, BBox: [[0.9677504897117615, 0.9863131046295166, 73.71336364746094, 161.01695251464844]]\n",
      "Detected in cropped_bread.jpg_566_318.jpg: cup, Confidence: 0.27, BBox: [[1.2759376764297485, 3.0769736766815186, 73.64950561523438, 161.5373077392578]]\n",
      "\n",
      "0: 448x640 (no detections), 7.6ms\n",
      "Speed: 2.5ms preprocess, 7.6ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 544x640 (no detections), 8.4ms\n",
      "Speed: 1.4ms preprocess, 8.4ms inference, 0.3ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 320x640 (no detections), 288.4ms\n",
      "Speed: 1.6ms preprocess, 288.4ms inference, 0.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 640x640 (no detections), 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 (no detections), 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x448 (no detections), 368.2ms\n",
      "Speed: 1.7ms preprocess, 368.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 (no detections), 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 608x640 (no detections), 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 0.3ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 384x640 (no detections), 244.5ms\n",
      "Speed: 1.8ms preprocess, 244.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x576 1 person, 346.4ms\n",
      "Speed: 2.4ms preprocess, 346.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Detected in cropped_coffee.jpg_302_242.jpg: person, Confidence: 0.46, BBox: [[1.6411603689193726, 2.601309061050415, 46.40191650390625, 29.793973922729492]]\n",
      "\n",
      "0: 480x640 (no detections), 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 remote, 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Detected in cropped_coffee.jpg_238_222.jpg: remote, Confidence: 0.45, BBox: [[10.036602020263672, 5.843038082122803, 40.5898323059082, 24.8365421295166]]\n",
      "\n",
      "0: 640x352 (no detections), 393.6ms\n",
      "Speed: 0.9ms preprocess, 393.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x608 (no detections), 8.1ms\n",
      "Speed: 2.4ms preprocess, 8.1ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 608x640 (no detections), 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 0.3ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 640x480 1 person, 6.2ms\n",
      "Speed: 2.1ms preprocess, 6.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected in cropped_coffee.jpg_2_339.jpg: person, Confidence: 0.38, BBox: [[0.03055114671587944, 3.265113353729248, 97.650634765625, 140.20396423339844]]\n",
      "\n",
      "0: 640x448 (no detections), 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 320x640 1 cat, 7.7ms\n",
      "Speed: 1.1ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Detected in cropped_coffee.jpg_402_102.jpg: cat, Confidence: 0.31, BBox: [[131.45846557617188, 4.552511692047119, 155.98883056640625, 42.416709899902344]]\n",
      "\n",
      "0: 640x416 1 cup, 6.2ms\n",
      "Speed: 1.9ms preprocess, 6.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Detected in cropped_coffecup.jpg_298_86.jpg: cup, Confidence: 0.76, BBox: [[0.2412872314453125, 0.694232165813446, 70.60912322998047, 110.92855072021484]]\n",
      "\n",
      "0: 480x640 (no detections), 6.2ms\n",
      "Speed: 1.9ms preprocess, 6.2ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 608x640 (no detections), 6.1ms\n",
      "Speed: 1.9ms preprocess, 6.1ms inference, 0.3ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 480x640 1 cat, 6.1ms\n",
      "Speed: 0.9ms preprocess, 6.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Detected in cropped_coffecup.jpg_70_234.jpg: cat, Confidence: 0.37, BBox: [[2.9132888317108154, 1.6398429870605469, 73.44384765625, 50.352783203125]]\n",
      "\n",
      "0: 640x512 1 cell phone, 240.3ms\n",
      "Speed: 1.3ms preprocess, 240.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Detected in cropped_coffecup.jpg_302_238.jpg: cell phone, Confidence: 0.33, BBox: [[0.611140251159668, 0.3399667739868164, 47.55602264404297, 45.25654983520508]]\n",
      "\n",
      "0: 480x640 1 cake, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Detected in cropped_coffecup.jpg_226_190.jpg: cake, Confidence: 0.53, BBox: [[10.322086334228516, 5.315003871917725, 34.50339126586914, 21.760038375854492]]\n",
      "\n",
      "0: 640x576 (no detections), 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 640x352 (no detections), 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x544 (no detections), 240.4ms\n",
      "Speed: 1.2ms preprocess, 240.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x576 (no detections), 6.2ms\n",
      "Speed: 2.5ms preprocess, 6.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 576)\n",
      "\n",
      "0: 352x640 (no detections), 6.1ms\n",
      "Speed: 1.7ms preprocess, 6.1ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 416x640 1 keyboard, 6.1ms\n",
      "Speed: 1.8ms preprocess, 6.1ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Detected in cropped_coffecup.jpg_362_218.jpg: keyboard, Confidence: 0.54, BBox: [[6.205053329467773, 4.14497709274292, 42.35838317871094, 25.50717544555664]]\n",
      "\n",
      "0: 640x480 (no detections), 6.2ms\n",
      "Speed: 1.1ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 480x640 1 keyboard, 6.2ms\n",
      "Speed: 1.1ms preprocess, 6.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Detected in cropped_coffecup.jpg_242_222.jpg: keyboard, Confidence: 0.62, BBox: [[4.897900581359863, 6.3838324546813965, 36.54547119140625, 25.082475662231445]]\n",
      "\n",
      "0: 640x448 (no detections), 7.0ms\n",
      "Speed: 1.1ms preprocess, 7.0ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 512x640 (no detections), 293.7ms\n",
      "Speed: 1.1ms preprocess, 293.7ms inference, 0.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 320x640 1 cat, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Detected in cropped_coffecup.jpg_402_102.jpg: cat, Confidence: 0.33, BBox: [[130.9506378173828, 3.7184481620788574, 155.96990966796875, 42.93422317504883]]\n",
      "\n",
      "0: 640x416 (no detections), 6.1ms\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 448x640 (no detections), 6.4ms\n",
      "Speed: 2.3ms preprocess, 6.4ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 (no detections), 6.9ms\n",
      "Speed: 0.9ms preprocess, 6.9ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 (no detections), 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 352x640 (no detections), 6.9ms\n",
      "Speed: 0.9ms preprocess, 6.9ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x608 (no detections), 7.9ms\n",
      "Speed: 2.3ms preprocess, 7.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 416x640 1 keyboard, 6.0ms\n",
      "Speed: 1.8ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Detected in cropped_coffeemachine.jpg_238_222.jpg: keyboard, Confidence: 0.62, BBox: [[9.607283592224121, 6.272298812866211, 41.36198806762695, 24.587421417236328]]\n",
      "\n",
      "0: 640x480 1 person, 7.1ms\n",
      "Speed: 1.3ms preprocess, 7.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected in cropped_coffeemachine.jpg_2_340.jpg: person, Confidence: 0.41, BBox: [[0.0, 1.227411150932312, 92.31607055664062, 139.33363342285156]]\n",
      "\n",
      "0: 640x608 1 person, 6.1ms\n",
      "Speed: 1.3ms preprocess, 6.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Detected in cropped_coffeemachine.jpg_298_242.jpg: person, Confidence: 0.46, BBox: [[6.149919033050537, 2.621758222579956, 50.13969421386719, 29.8063907623291]]\n",
      "\n",
      "0: 640x480 1 person, 6.9ms\n",
      "Speed: 1.1ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected in cropped_coffeemachine.jpg_578_394.jpg: person, Confidence: 0.42, BBox: [[1.6596221923828125, 5.000777244567871, 61.56446838378906, 85.66346740722656]]\n",
      "\n",
      "0: 512x640 (no detections), 6.1ms\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 320x640 1 bowl, 7.3ms\n",
      "Speed: 1.0ms preprocess, 7.3ms inference, 0.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Detected in cropped_coffeemachine.jpg_430_82.jpg: bowl, Confidence: 0.26, BBox: [[1.5810860395431519, 0.06949786841869354, 196.0793914794922, 90.74279022216797]]\n",
      "\n",
      "0: 640x608 (no detections), 6.2ms\n",
      "Speed: 1.7ms preprocess, 6.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 608)\n",
      "\n",
      "0: 576x640 1 cat, 6.0ms\n",
      "Speed: 1.1ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Detected in cropped_coffeemachine.jpg_414_82.jpg: cat, Confidence: 0.58, BBox: [[18.634069442749023, 3.3209705352783203, 218.97903442382812, 90.77217102050781]]\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 tv, 6.9ms\n",
      "Speed: 1.3ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Detected in cropped_coffeemachine.jpg_530_102.jpg: tv, Confidence: 0.63, BBox: [[0.1258508563041687, 13.34775447845459, 19.04237937927246, 47.347843170166016]]\n",
      "\n",
      "0: 640x416 (no detections), 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 608x640 1 cake, 7.0ms\n",
      "Speed: 1.2ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Detected in cropped_plate.jpg_282_198.jpg: cake, Confidence: 0.62, BBox: [[0.1998424381017685, 1.7933892011642456, 74.69994354248047, 65.90593719482422]]\n",
      "\n",
      "0: 480x640 1 apple, 1 orange, 6.2ms\n",
      "Speed: 1.0ms preprocess, 6.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Detected in cropped_plate.jpg_490_194.jpg: orange, Confidence: 0.40, BBox: [[0.27543649077415466, 0.1412479430437088, 77.91837310791016, 57.47385787963867]]\n",
      "Detected in cropped_plate.jpg_490_194.jpg: apple, Confidence: 0.36, BBox: [[0.25211116671562195, 0.15290307998657227, 78.67005157470703, 57.1673469543457]]\n",
      "\n",
      "0: 640x640 1 orange, 6.2ms\n",
      "Speed: 1.1ms preprocess, 6.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected in cropped_plate.jpg_218_178.jpg: orange, Confidence: 0.28, BBox: [[1.5354037284851074, 1.789974570274353, 49.67843246459961, 49.09749221801758]]\n",
      "\n",
      "0: 352x640 1 apple, 1 orange, 6.2ms\n",
      "Speed: 1.7ms preprocess, 6.2ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Detected in cropped_plate.jpg_374_170.jpg: apple, Confidence: 0.37, BBox: [[0.0, 0.7311971187591553, 82.89823150634766, 41.58311462402344]]\n",
      "Detected in cropped_plate.jpg_374_170.jpg: orange, Confidence: 0.27, BBox: [[0.07984255999326706, 1.3375946283340454, 82.6677017211914, 41.822601318359375]]\n",
      "\n",
      "0: 416x640 (no detections), 6.1ms\n",
      "Speed: 0.9ms preprocess, 6.1ms inference, 0.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 192x640 1 banana, 244.0ms\n",
      "Speed: 0.9ms preprocess, 244.0ms inference, 0.8ms postprocess per image at shape (1, 3, 192, 640)\n",
      "Detected in cropped_plate.jpg_286_278.jpg: banana, Confidence: 0.45, BBox: [[1.6517494916915894, 2.145526885986328, 292.5364074707031, 77.49504852294922]]\n",
      "\n",
      "0: 256x640 (no detections), 236.9ms\n",
      "Speed: 1.1ms preprocess, 236.9ms inference, 0.3ms postprocess per image at shape (1, 3, 256, 640)\n",
      "\n",
      "0: 640x640 (no detections), 8.2ms\n",
      "Speed: 1.9ms preprocess, 8.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 576x640 (no detections), 6.1ms\n",
      "Speed: 2.0ms preprocess, 6.1ms inference, 0.3ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x480 1 person, 7.3ms\n",
      "Speed: 1.0ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected in cropped_plate.jpg_2_339.jpg: person, Confidence: 0.28, BBox: [[0.17046517133712769, 4.276219367980957, 96.42192077636719, 140.55828857421875]]\n",
      "\n",
      "0: 224x640 (no detections), 242.8ms\n",
      "Speed: 0.9ms preprocess, 242.8ms inference, 0.3ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "0: 640x512 1 tv, 7.7ms\n",
      "Speed: 1.2ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Detected in cropped_plate.jpg_514_86.jpg: tv, Confidence: 0.25, BBox: [[14.292831420898438, 9.570090293884277, 31.914505004882812, 39.754852294921875]]\n",
      "\n",
      "0: 416x640 1 tv, 6.1ms\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Detected in cropped_plate.jpg_506_230.jpg: tv, Confidence: 0.58, BBox: [[73.02506256103516, 0.350989431142807, 131.75601196289062, 39.43135452270508]]\n",
      "\n",
      "0: 288x640 (no detections), 326.7ms\n",
      "Speed: 1.2ms preprocess, 326.7ms inference, 0.3ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 416x640 (no detections), 7.6ms\n",
      "Speed: 2.6ms preprocess, 7.6ms inference, 0.3ms postprocess per image at shape (1, 3, 416, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "image_folder = '/home/ttyh/hot3d/hot3d/dataset/cropped_objects'\n",
    "# Process each image in the folder\n",
    "for image_name in os.listdir(image_folder):\n",
    "    image_path = os.path.join(image_folder, image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "    results = model(image)[0]\n",
    "    for box in results.boxes:\n",
    "        label = results.names[int(box.cls)]\n",
    "        confidence = box.conf.item()\n",
    "        bbox = box.xyxy.tolist()\n",
    "        print(f\"Detected in {image_name}: {label}, Confidence: {confidence:.2f}, BBox: {bbox}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 bowl, 1 donut, 38.4ms\n",
      "Speed: 1.6ms preprocess, 38.4ms inference, 19.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Detected in bowl.jpg: bowl, Confidence: 0.83, BBox: [[265.8050842285156, 74.77810668945312, 389.0524597167969, 172.14791870117188]]\n",
      "Detected in bowl.jpg: donut, Confidence: 0.69, BBox: [[282.74169921875, 197.87307739257812, 357.4764404296875, 262.88568115234375]]\n",
      "\n",
      "0: 480x640 1 cup, 1 hot dog, 1 donut, 1 cell phone, 6.5ms\n",
      "Speed: 1.1ms preprocess, 6.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Detected in bread.jpg: hot dog, Confidence: 0.49, BBox: [[324.9237060546875, 241.66587829589844, 559.75341796875, 365.847412109375]]\n",
      "Detected in bread.jpg: cup, Confidence: 0.39, BBox: [[436.44183349609375, 0.0419769287109375, 547.627197265625, 157.53321838378906]]\n",
      "Detected in bread.jpg: donut, Confidence: 0.35, BBox: [[274.4947509765625, 205.23507690429688, 350.37939453125, 271.52032470703125]]\n",
      "Detected in bread.jpg: cell phone, Confidence: 0.25, BBox: [[576.8426513671875, 214.50558471679688, 639.7425537109375, 269.5503234863281]]\n",
      "\n",
      "0: 480x640 3 cups, 1 keyboard, 5.9ms\n",
      "Speed: 1.1ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Detected in coffee.jpg: cup, Confidence: 0.46, BBox: [[298.4302978515625, 239.53024291992188, 347.908203125, 295.3952941894531]]\n",
      "Detected in coffee.jpg: cup, Confidence: 0.46, BBox: [[236.489990234375, 222.291748046875, 284.6390380859375, 275.2662353515625]]\n",
      "Detected in coffee.jpg: cup, Confidence: 0.31, BBox: [[362.7169189453125, 218.1141357421875, 410.25384521484375, 271.76385498046875]]\n",
      "Detected in coffee.jpg: keyboard, Confidence: 0.30, BBox: [[581.18701171875, 372.6671142578125, 639.809814453125, 479.64837646484375]]\n",
      "\n",
      "0: 480x640 2 cups, 5.8ms\n",
      "Speed: 0.8ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Detected in coffecup.jpg: cup, Confidence: 0.84, BBox: [[298.0279541015625, 85.53075408935547, 368.65008544921875, 197.71087646484375]]\n",
      "Detected in coffecup.jpg: cup, Confidence: 0.45, BBox: [[236.67376708984375, 222.1988525390625, 284.90692138671875, 275.7572326660156]]\n",
      "\n",
      "0: 480x640 1 sink, 5.8ms\n",
      "Speed: 0.8ms preprocess, 5.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Detected in coffeemachine.jpg: sink, Confidence: 0.34, BBox: [[0.266357421875, 8.869049072265625, 590.6482543945312, 477.2770690917969]]\n",
      "\n",
      "0: 480x640 1 bowl, 2 donuts, 1 dining table, 5.8ms\n",
      "Speed: 0.8ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Detected in plate.jpg: bowl, Confidence: 0.86, BBox: [[266.0908203125, 74.6995849609375, 388.909912109375, 171.7568817138672]]\n",
      "Detected in plate.jpg: donut, Confidence: 0.75, BBox: [[282.39935302734375, 197.7249755859375, 356.949462890625, 261.96929931640625]]\n",
      "Detected in plate.jpg: donut, Confidence: 0.56, BBox: [[487.96478271484375, 192.74533081054688, 569.6647338867188, 253.67623901367188]]\n",
      "Detected in plate.jpg: dining table, Confidence: 0.42, BBox: [[1.090087890625, 21.8958740234375, 634.087158203125, 479.4373779296875]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "image_folder = '/home/ttyh/hot3d/hot3d/dataset/images'\n",
    "# Process each image in the folder\n",
    "for image_name in os.listdir(image_folder):\n",
    "    image_path = os.path.join(image_folder, image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "    results = model(image)[0]\n",
    "    for box in results.boxes:\n",
    "        label = results.names[int(box.cls)]\n",
    "        confidence = box.conf.item()\n",
    "        bbox = box.xyxy.tolist()\n",
    "        print(f\"Detected in {image_name}: {label}, Confidence: {confidence:.2f}, BBox: {bbox}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd0d41e806c4db5a0b7502264d996a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ttyh/hot3d/hot3d/dataset/images/bowl.jpg : In the image, there are several objects on a table. These include a bowl, a black cutting mat, a piece of bread, a piece of cake, a piece of fruit, and a piece of meat.\n",
      "/home/ttyh/hot3d/hot3d/dataset/images/bread.jpg : In the image, there are several objects:\n",
      "\n",
      "1. A machine with a black base and a silver top, possibly a 3D printer or a machine for creating objects.\n",
      "2. A black cutting mat with white grid lines, used for cutting or guiding the placement of objects.\n",
      "3. A black tray with a grid pattern, which is likely used to hold objects in place during the printing or cutting process.\n",
      "4. A white plate, which might be used to hold or display the objects after they are created.\n",
      "5. A white object that appears to be a piece of paper or a label, placed on the cutting mat.\n",
      "6. A white object that looks like a piece of fruit, possibly a banana or a similar item, placed on the cutting mat.\n",
      "7. A white object that resembles a piece of bread or a roll, placed on the cutting mat.\n",
      "8. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "9. A white object that appears to be a piece of cheese or a slice of bread, placed on the cutting mat.\n",
      "10. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "11. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "12. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "13. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "14. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "15. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "16. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "17. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "18. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "19. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "20. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "21. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "22. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "23. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "24. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "25. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "26. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "27. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "28. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "29. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "30. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "31. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "32. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "33. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "34. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "35. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "36. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "37. A white object that appears to be a piece of meat or a sausage, placed on the cutting mat.\n",
      "38. A white object that looks like a piece of meat or a sausage, placed on the cutting mat.\n",
      "39. A white object that appears to be a piece of meat or a sausage, placed on\n",
      "/home/ttyh/hot3d/hot3d/dataset/images/coffee.jpg : In the image, there is a robotic arm, a white cylindrical object, a table with stickers, and a cup with a sticker on it.\n",
      "/home/ttyh/hot3d/hot3d/dataset/images/coffecup.jpg : In the image, there is a white table with a cup on it. On the table, there are also several stickers with different designs and colors. Additionally, there is a white pedestal with a black base and a white cup on it.\n",
      "/home/ttyh/hot3d/hot3d/dataset/images/coffeemachine.jpg : In the image, there is a microscope with a white base and a black stand. On the microscope, there are six petri dishes with different colored labels. There is also a white cup with a black lid on the right side of the microscope.\n",
      "/home/ttyh/hot3d/hot3d/dataset/images/plate.jpg : In the image, there is a white plate with a long, orange-colored bread on it, a small white bowl, a black cutting board with four pieces of bread, and a machine with a black handle and a silver base.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, AutoModelForCausalLM\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"CPU\"\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-vision-instruct\" \n",
    "\n",
    "# Note: set _attn_implementation='eager' if you don't have flash_attn installed\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id, \n",
    "  device_map=\"cuda\", \n",
    "  trust_remote_code=True, \n",
    "  torch_dtype=\"auto\", \n",
    "  _attn_implementation='flash_attention_2'    \n",
    ")\n",
    "\n",
    "# for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\n",
    "processor = AutoProcessor.from_pretrained(model_id, \n",
    "  trust_remote_code=True, \n",
    "  num_crops=4\n",
    ") \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"<|image_1|>\\n List the objects in this picture\"},\n",
    "]\n",
    "\n",
    "prompt = processor.tokenizer.apply_chat_template(\n",
    "  messages, \n",
    "  tokenize=False, \n",
    "  add_generation_prompt=True\n",
    ")\n",
    "\n",
    "directory = '/home/ttyh/hot3d/hot3d/dataset/images'\n",
    "for file in os.listdir(directory):\n",
    "    image_path = os.path.join(directory, file)\n",
    "    img = Image.open(image_path)\n",
    "    inputs = processor(prompt, img, return_tensors=\"pt\").to(\"cuda:0\") \n",
    "    \n",
    "    generation_args = { \n",
    "        \"max_new_tokens\": 1000, \n",
    "        \"temperature\": 0.0, \n",
    "        \"do_sample\": False, \n",
    "    } \n",
    "    \n",
    "    generate_ids = model.generate(**inputs, \n",
    "      eos_token_id=processor.tokenizer.eos_token_id, \n",
    "      **generation_args\n",
    "    )\n",
    "    \n",
    "    # remove input tokens \n",
    "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "    response = processor.batch_decode(generate_ids, \n",
    "      skip_special_tokens=True, \n",
    "      clean_up_tokenization_spaces=False)[0] \n",
    "    \n",
    "    print(image_path, ':', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544429a364ef44a0a7a9c279aff47525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ttyh/hot3d/hot3d/dataset/box/fast_sambowl.jpg.jpg : In the image, there are several objects that can be identified. Starting from the left, there is a white bowl on a black mat. Next to the bowl, there is a white container with a black lid, which appears to be a blender or a mixer. In the center of the image, there is a large, elongated object that resembles a hot dog or a sausage. To the right of the hot dog, there is a smaller, round object that could be a donut or a pastry. The background is less clear, but it seems to be a kitchen or a workspace with various items and equipment.\n",
      "/home/ttyh/hot3d/hot3d/dataset/box/fast_sambread.jpg.jpg : The image shows a laboratory setting with various objects. There are two objects that resemble donuts, one on the left side of the image and another on the right side. There is also a long, orange object that could be a piece of food or a test tube, located in the center of the image. Additionally, there are green boxes with white outlines, possibly indicating areas of interest or measurements, and a black mat with a grid pattern on the table.\n",
      "/home/ttyh/hot3d/hot3d/dataset/box/fast_samcoffecup.jpg.jpg : The image shows a variety of objects on a table, which are partially obscured by green bounding boxes. From the visible parts, we can identify a white cylindrical object that could be a container or a piece of equipment, a black rectangular object that might be a device or a tool, and a white object with a red label that could be a container or a part of a machine. The exact nature of these objects is not clear due to the partial view and the obstruction caused by the green boxes.\n",
      "/home/ttyh/hot3d/hot3d/dataset/box/fast_samcoffee.jpg.jpg : The image shows a robotic arm with a gripper, a white cylindrical object that could be a part of a machine or a container, and several objects on a flat surface. The objects on the surface are within green bounding boxes and include a can with a label, a container with a red lid, a box with a white lid, and a cylindrical object with a black top.\n",
      "/home/ttyh/hot3d/hot3d/dataset/box/fast_samcoffeemachine.jpg.jpg : The image shows a variety of objects on a table. There are several stickers with different designs, a can of soda, a cup, and what appears to be a part of a machine or equipment with a black and white color scheme. The exact nature of the machine or equipment is not clear from the image.\n",
      "/home/ttyh/hot3d/hot3d/dataset/box/fast_samplate.jpg.jpg : In the image, there are several objects that can be identified. Starting from the left, there is a white bowl on a black cutting board. Next to it, there is a piece of bread on the cutting board. Moving to the right, there is a piece of orange fruit, which appears to be a peeled orange. Further to the right, there is another piece of orange fruit, which seems to be a peeled orange as well. In the background, there is a white container, possibly a blender or a mixer, and a white bowl. The objects are placed on a white countertop.\n",
      "/home/ttyh/hot3d/hot3d/dataset/box/mobilesam_bowl.jpg.jpg : In the image, there are several objects that can be identified. Starting from the left, there is a white bowl, a white cup, a white plate, a white container, a white bottle, a white cup, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle, a white container, a white bottle,\n",
      "/home/ttyh/hot3d/hot3d/dataset/box/mobilesam_bread.jpg.jpg : The image shows a variety of objects on a table. There are several food items, including what appears to be a bagel, a slice of bread, and a piece of fruit. There are also some green boxes with white outlines, possibly containing labels or markers. Additionally, there is a white cylindrical object that could be a container or a piece of equipment.\n",
      "/home/ttyh/hot3d/hot3d/dataset/box/mobilesam_coffecup.jpg.jpg : The image shows a variety of objects on a table. There are several cans of food, including what appears to be canned vegetables and possibly some canned meats. There is also a can of beer, a can of soda, and a can of soup. Additionally, there are some other items that are not clearly identifiable due to the angle and partial view, such as a black object that could be a phone or a remote control, and a white object that might be a cup or a container.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, AutoModelForCausalLM\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"CPU\"\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-vision-instruct\" \n",
    "\n",
    "# Note: set _attn_implementation='eager' if you don't have flash_attn installed\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id, \n",
    "  device_map=\"cuda\", \n",
    "  trust_remote_code=True, \n",
    "  torch_dtype=\"auto\", \n",
    "  _attn_implementation='flash_attention_2'    \n",
    ")\n",
    "\n",
    "# for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\n",
    "processor = AutoProcessor.from_pretrained(model_id, \n",
    "  trust_remote_code=True, \n",
    "  num_crops=4\n",
    ") \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"<|image_1|>\\n List the objects in this picture, especially those in the bounding boxes.\"},\n",
    "]\n",
    "\n",
    "prompt = processor.tokenizer.apply_chat_template(\n",
    "  messages, \n",
    "  tokenize=False, \n",
    "  add_generation_prompt=True\n",
    ")\n",
    "\n",
    "directory = '/home/ttyh/hot3d/hot3d/dataset/box'\n",
    "for file in os.listdir(directory):\n",
    "    image_path = os.path.join(directory, file)\n",
    "    img = Image.open(image_path)\n",
    "    inputs = processor(prompt, img, return_tensors=\"pt\").to(\"cuda:0\") \n",
    "    \n",
    "    generation_args = { \n",
    "        \"max_new_tokens\": 1000, \n",
    "        \"temperature\": 0.0, \n",
    "        \"do_sample\": False, \n",
    "    } \n",
    "    \n",
    "    generate_ids = model.generate(**inputs, \n",
    "      eos_token_id=processor.tokenizer.eos_token_id, \n",
    "      **generation_args\n",
    "    )\n",
    "    \n",
    "    # remove input tokens \n",
    "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "    response = processor.batch_decode(generate_ids, \n",
    "      skip_special_tokens=True, \n",
    "      clean_up_tokenization_spaces=False)[0] \n",
    "    \n",
    "    print(image_path, ':', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ttyh/hot3d/hot3d/dataset/images/bread.jpg: 480x640 1 cup, 1 banana, 1 hot dog, 3 donuts, 1 dining table, 42.4ms\n",
      "Speed: 5.7ms preprocess, 42.4ms inference, 110.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model (ensure you have the model downloaded)\n",
    "model = YOLO(\"yolov8n-seg.pt\")  # Use a segmentation model\n",
    "\n",
    "# Read the image\n",
    "image_path = '/home/ttyh/hot3d/hot3d/dataset/images/bread.jpg'\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Perform segmentation\n",
    "results = model(image_path)\n",
    "\n",
    "# Draw segmentation masks and bounding boxes\n",
    "for result in results:\n",
    "    masks = result.masks.xy  # Get segmentation masks\n",
    "    for mask in masks:\n",
    "        mask = mask.astype(int)\n",
    "        cv2.polylines(img, [mask], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "# Display segmented image\n",
    "cv2.imshow(\"Segmented Image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
